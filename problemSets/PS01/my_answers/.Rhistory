source("~/Desktop/asds/stats2/StatsII_Spring2024/problemSets/PS01/my_answers/PS1 copy.R")
## Packages
library(tidyverse) # load our packages here
library(rvest)
library(xml2)
# We use the read_html() function from rvest to read in the html
bowlers <- "https://stats.espncricinfo.com/ci/content/records/93276.html"
html <- read_html(bowlers)
html
# We can inspect the structure of this html using xml_structure() from xml2
xml_structure(html)
capture.output(xml_structure(html))
# html nodes using html_nodes()
html %>%
html_nodes() # try searching for the table node
"table"
".ds-table"
xpath = "//table[position() = 1]"
xpath = "//table/thead | //table/tbody"
html %>%
html_nodes() # try searching using the class (add a dot)
# xpaths
# To search using xpath selectors, we need to add the xpath argument.
html %>%
html_nodes(xpath = "//table")
# Try selecting the first node of the table class, and assign it to a new object
tab1 <- html %>%
html_nodes()
# We basically want "thead" and "tbody". How might we get those?
tab2 <- tab1 %>%
html_nodes()
# We now have an object containing 2 lists. With a bit of work we can extract
# the text we want as a vector:
heads <- tab2[1] %>%
html_nodes(xpath = "") %>%
html_text()
body <- tab2[2] %>%
html_nodes(xpath = "") %>%
html_text()
library(rvest)
library(polite)
library(rvest)
library(polite)
url <-"https://www.mfa.gr/en/current-affairs/statements-speeches/?page=0" #add the URL here as a string
# Acquire the html locally (we use the polite package, because the website does not allow anonymous scraping)
page <- url %>%
bow() %>%
scrape() # parse page into XML structure
library(rvest)
library(polite)
url <-"https://www.mfa.gr/en/current-affairs/statements-speeches/?page=0" #add the URL here as a string
# Acquire the html locally (we use the polite package, because the website does not allow anonymous scraping)
page <- url %>%
bow() %>%
scrape() # parse page into XML structure
# get hyperlinks
hyperlinks <- page %>%
html_node(".tcs") %>%
html_nodes(css = "") %>% # which list node are the links in?
html_node("") %>% # which child node here?
html_attr(name = "") # what is the name of the attribute we need?
install.packages("polite")
library(polite)
url <-"https://www.mfa.gr/en/current-affairs/statements-speeches/?page=0" #add the URL here as a string
# Acquire the html locally (we use the polite package, because the website does not allow anonymous scraping)
page <- url %>%
bow() %>%
scrape() # parse page into XML structure
url <-"https://www.mfa.gr/en/current-affairs/statements-speeches/?page=0.html" #add the URL here as a string
# Acquire the html locally (we use the polite package, because the website does not allow anonymous scraping)
page <- url %>%
bow() %>%
scrape() # parse page into XML structure
library(rvest)
library(polite)
library(rvest)
library(polite)
url <-"https://www.mfa.gr/en/current-affairs/statements-speeches/?page=0" #add the URL here as a string
# Acquire the html locally (we use the polite package, because the website does not allow anonymous scraping)
page <- url %>%
bow() %>%
scrape() # parse page into XML structure
session <- bow(url) # Create a session using the bow function
page <- scrape(session) # Scrape the page content
View(session)
url <-"https://www.mfa.gr/en/current-affairs/statements-speeches/?page=0.html" #add the URL here as a string
# Acquire the html locally (we use the polite package, because the website does not allow anonymous scraping)
##THE BELOW GIVEN CODE DIDNT WORK FOR ME, TRYING SOMETHING ELSE
"""page <- url %>%
url <-"https://www.mfa.gr/en/current-affairs/statements-speeches/?page=0.html" #add the URL here as a string
url <-"https://www.mfa.gr/en/current-affairs/statements-speeches/?page=0.html" #add the URL here as a string
# get hyperlinks
hyperlinks <- page %>%
html_node(".tcs") %>%
html_nodes(css = "") %>% # which list node are the links in?
html_node("") %>% # which child node here?
html_attr(name = "") # what is the name of the attribute we need?
session <- bow(url) # Create a session using the bow function
page <- scrape(session) # Scrape the page content
# get hyperlinks
hyperlinks <- page %>%
html_node(".tcs") %>%
html_nodes(css = "") %>% # which list node are the links in?
html_node("") %>% # which child node here?
html_attr(name = "") # what is the name of the attribute we need?
# Acquire the html locally (we use the polite package, because the website does not allow anonymous scraping)
#page <- url %>%
# bow() %>%
#scrape() # parse page into XML structure
session <- bow(url) %>%
user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36")
library(httr)
# Acquire the html locally (we use the polite package, because the website does not allow anonymous scraping)
#page <- url %>%
# bow() %>%
#scrape() # parse page into XML structure
session <- bow(url)
response <- session %>%
httr::GET(user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"))
url <-"https://www.mfa.gr/en/current-affairs/statements-speeches/?page=0" #add the URL here as a string
# Acquire the html locally (we use the polite package, because the website does not allow anonymous scraping)
#page <- url %>%
# bow() %>%
#scrape() # parse page into XML structure
session <- bow(url)
response <- session %>%
httr::GET(user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"))
url <-"https://www.mfa.gr/en/current-affairs/statements-speeches/?page=0" #add the URL here as a string
html <- read_html(url)
url <-"https://www.mfa.gr/en/current-affairs/statements-speeches/?page=0.html" #add the URL here as a string
html <- read_html(url)
