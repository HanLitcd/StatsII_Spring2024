source("~/Desktop/asds/stats2/StatsII_Spring2024/problemSets/PS01/my_answers/PS1 copy.R")
## Packages
library(tidyverse) # load our packages here
library(rvest)
library(xml2)
# We use the read_html() function from rvest to read in the html
bowlers <- "https://stats.espncricinfo.com/ci/content/records/93276.html"
html <- read_html(bowlers)
html
# We can inspect the structure of this html using xml_structure() from xml2
xml_structure(html)
capture.output(xml_structure(html))
# html nodes using html_nodes()
html %>%
html_nodes() # try searching for the table node
"table"
".ds-table"
xpath = "//table[position() = 1]"
xpath = "//table/thead | //table/tbody"
html %>%
html_nodes() # try searching using the class (add a dot)
# xpaths
# To search using xpath selectors, we need to add the xpath argument.
html %>%
html_nodes(xpath = "//table")
# Try selecting the first node of the table class, and assign it to a new object
tab1 <- html %>%
html_nodes()
# We basically want "thead" and "tbody". How might we get those?
tab2 <- tab1 %>%
html_nodes()
# We now have an object containing 2 lists. With a bit of work we can extract
# the text we want as a vector:
heads <- tab2[1] %>%
html_nodes(xpath = "") %>%
html_text()
body <- tab2[2] %>%
html_nodes(xpath = "") %>%
html_text()
library(rvest)
library(polite)
library(rvest)
library(polite)
url <-"https://www.mfa.gr/en/current-affairs/statements-speeches/?page=0" #add the URL here as a string
# Acquire the html locally (we use the polite package, because the website does not allow anonymous scraping)
page <- url %>%
bow() %>%
scrape() # parse page into XML structure
library(rvest)
library(polite)
url <-"https://www.mfa.gr/en/current-affairs/statements-speeches/?page=0" #add the URL here as a string
# Acquire the html locally (we use the polite package, because the website does not allow anonymous scraping)
page <- url %>%
bow() %>%
scrape() # parse page into XML structure
# get hyperlinks
hyperlinks <- page %>%
html_node(".tcs") %>%
html_nodes(css = "") %>% # which list node are the links in?
html_node("") %>% # which child node here?
html_attr(name = "") # what is the name of the attribute we need?
install.packages("polite")
library(polite)
url <-"https://www.mfa.gr/en/current-affairs/statements-speeches/?page=0" #add the URL here as a string
# Acquire the html locally (we use the polite package, because the website does not allow anonymous scraping)
page <- url %>%
bow() %>%
scrape() # parse page into XML structure
url <-"https://www.mfa.gr/en/current-affairs/statements-speeches/?page=0.html" #add the URL here as a string
# Acquire the html locally (we use the polite package, because the website does not allow anonymous scraping)
page <- url %>%
bow() %>%
scrape() # parse page into XML structure
library(rvest)
library(polite)
library(rvest)
library(polite)
url <-"https://www.mfa.gr/en/current-affairs/statements-speeches/?page=0" #add the URL here as a string
# Acquire the html locally (we use the polite package, because the website does not allow anonymous scraping)
page <- url %>%
bow() %>%
scrape() # parse page into XML structure
session <- bow(url) # Create a session using the bow function
page <- scrape(session) # Scrape the page content
View(session)
url <-"https://www.mfa.gr/en/current-affairs/statements-speeches/?page=0.html" #add the URL here as a string
# Acquire the html locally (we use the polite package, because the website does not allow anonymous scraping)
##THE BELOW GIVEN CODE DIDNT WORK FOR ME, TRYING SOMETHING ELSE
"""page <- url %>%
url <-"https://www.mfa.gr/en/current-affairs/statements-speeches/?page=0.html" #add the URL here as a string
url <-"https://www.mfa.gr/en/current-affairs/statements-speeches/?page=0.html" #add the URL here as a string
# get hyperlinks
hyperlinks <- page %>%
html_node(".tcs") %>%
html_nodes(css = "") %>% # which list node are the links in?
html_node("") %>% # which child node here?
html_attr(name = "") # what is the name of the attribute we need?
session <- bow(url) # Create a session using the bow function
page <- scrape(session) # Scrape the page content
# get hyperlinks
hyperlinks <- page %>%
html_node(".tcs") %>%
html_nodes(css = "") %>% # which list node are the links in?
html_node("") %>% # which child node here?
html_attr(name = "") # what is the name of the attribute we need?
# Acquire the html locally (we use the polite package, because the website does not allow anonymous scraping)
#page <- url %>%
# bow() %>%
#scrape() # parse page into XML structure
session <- bow(url) %>%
user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36")
library(httr)
# Acquire the html locally (we use the polite package, because the website does not allow anonymous scraping)
#page <- url %>%
# bow() %>%
#scrape() # parse page into XML structure
session <- bow(url)
response <- session %>%
httr::GET(user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"))
url <-"https://www.mfa.gr/en/current-affairs/statements-speeches/?page=0" #add the URL here as a string
# Acquire the html locally (we use the polite package, because the website does not allow anonymous scraping)
#page <- url %>%
# bow() %>%
#scrape() # parse page into XML structure
session <- bow(url)
response <- session %>%
httr::GET(user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"))
url <-"https://www.mfa.gr/en/current-affairs/statements-speeches/?page=0" #add the URL here as a string
html <- read_html(url)
url <-"https://www.mfa.gr/en/current-affairs/statements-speeches/?page=0.html" #add the URL here as a string
html <- read_html(url)
source("~/Desktop/asds/stats2/StatsII_Spring2024/problemSets/PS01/my_answers/PS1 copy.R")
source("~/Desktop/asds/stats2/StatsII_Spring2024/problemSets/PS01/my_answers/PS1 copy.R")
#check
ks.test(data,'pnorm')
source("~/Desktop/asds/stats2/StatsII_Spring2024/problemSets/PS01/my_answers/PS1 copy.R")
#check
ks.test(data1,'pnorm')
# Starting values for the coefficients
bfgs_model <- optim(logLikFun, data = data, method = "BFGS",hessian = T)
beta
data
bfgs_model <- optim(logLikFun, beta=start_values, data = data, method = "BFGS",hessian = T)
bfgs_model <- optim(fn=logLikFun, par=start_values, data = data, method = "BFGS",hessian = T)
# Starting values for the coefficients
start_values <- c(0,1)
bfgs_model <- optim(fn=logLikFun, par=start_values, data = data, method = "BFGS",hessian = T)
# Output the coefficients from both models
list(lm_model = coef(lm_model), bfgs_model = bfgs_model$par)
source("~/Desktop/asds/stats2/StatsII_Spring2024/problemSets/PS01/my_answers/PS1 copy.R")
par=c(1,2,3)
source("~/Desktop/asds/stats2/StatsII_Spring2024/problemSets/PS01/my_answers/PS01_answersHanLi.R")
source("~/Desktop/asds/stats2/StatsII_Spring2024/problemSets/PS01/my_answers/PS01_answersHanLi.R")
data
source("~/Desktop/asds/stats2/StatsII_Spring2024/problemSets/PS01/my_answers/PS01_answersHanLi.R")
# Output the coefficients from both models
list(lm_model = coef(lm_model), bfgs_model = bfgs_model$par)
#check
ks.test(data1,'pnorm')
knitr::opts_chunk$set(echo = TRUE, comment = "")
library(lme4)
library(splines)
library(ggplot2)
library(dplyr)
library(readr)
library(stargazer)
library(readxl)
library(lubridate)
library(broom)
library(tidyr)
library(cowplot)
install.packages("cowplot")
library(lme4)
library(splines)
library(ggplot2)
library(dplyr)
library(readr)
library(stargazer)
library(readxl)
library(lubridate)
library(broom)
library(tidyr)
library(cowplot)
library(xtable)
library(quanteda) #tested on 1.2 or 1.3, 0.9 is not the same output
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_chunk$set(error = TRUE)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
# Create the corpus from the 'text' column, and use the 'sentiment' column as a document variable
corpus <- corpus(data, text_field = "text")
View(data)
data <- read.csv("yelp_data_small.csv",
stringsAsFactors=FALSE,
encoding = "utf-8")
View(data)
View(data)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
# Create the corpus from the 'text' column, and use the 'sentiment' column as a document variable
corpus <- corpus(data, text_field = "text")
# Assign the 'sentiment' column from data frame as a document variable
docvars(corpus, "sentiment") <- data$sentiment
# Inspect the corpus to confirm 'sentiment' is correctly assigned
summary(corpus)
head(docvars(corpus))
# Calculate the distribution of the 'sentiment' variable
sentiment_distribution <- table(docvars(corpus, "sentiment"))
# Calculate probabilities
sentiment_probability <- sentiment_distribution / sum(sentiment_distribution)
# Print the distribution and probabilities
print(sentiment_distribution)
print(sentiment_probability)
# Specifically for the "positive" class, assuming "pos" represents positive
positive_probability <- sentiment_probability["pos"]
print(paste("Probability of 'pos' (positive) class:", positive_probability))
# Assess if classes are balanced
if (max(sentiment_probability) - min(sentiment_probability) < 0.1) {
print("Classes are relatively balanced.")
} else {
print("Classes are imbalanced.")
}
# Your code here
# 1. Tokenization and initial cleaning
tokens <- tokens(corpus, what = "word", remove_punct = TRUE, remove_symbols = TRUE)
# 2. Stopwords Removal
tokens <- tokens_remove(tokens, stopwords("en"))
# 3. Stemming
# 4. Create a DFM, removing rare tokens
dfm <- dfm(tokens, remove = stopwords("en"), remove_numbers = TRUE, remove_punct = TRUE)
dfm <- dfm_trim(dfm, min_termfreq = 10, termfreq_type = "count")
# Apply tf-idf weighting
dfm <- dfm_tfidf(dfm)
# Inspect the DFM
#print(dfm)
#print(dfm_tfidf)
# Your code here
# 1. Tokenization and initial cleaning
tokens <- tokens(corpus, what = "word", remove_punct = TRUE, remove_symbols = TRUE)
# 2. Stopwords Removal
tokens <- tokens_remove(tokens, stopwords("en"))
# 3. Stemming
tokens <- tokens_wordstem(tokens)
# 4. Create a DFM, removing rare tokens
dfm <- dfm(tokens, remove = stopwords("en"), remove_numbers = TRUE, remove_punct = TRUE)
dfm <- dfm_trim(dfm, min_termfreq = 10, termfreq_type = "count")
# Apply tf-idf weighting
dfm <- dfm_tfidf(dfm)
# Inspect the DFM
#print(dfm)
#print(dfm_tfidf)
library(caret)
install.packages("caret")
library(caret)
tmpdata <- as.data.frame(as.matrix(dfm)) # convert the dfm object to a data.frame
tmpdata <- tmpdata[, -1] # drop document id variable (first variable)
sentiment <- as.factor(docvars(corpus, "sentiment")) # extract sentiment labels from the dfm object (hint: the dfm is an S4 class object)
ldata <- cbind(sentiment, tmpdata) # bind sentiment and tmpdata to create a labelled data frame
train_row_nums <- createDataPartition(y = ldata$sentiment, # set sentiment as the Y variable in caret
p = 0.8, # fill in the blank
list=FALSE)
Train <- ldata[train_row_nums, ] # use train_row_nums to subset ldata and create the training set
Test <- ldata[-train_row_nums, ] # train_row_nums to subset ldata and create the testing set
#install.packages("naivebayes")
library('doParallel') # for parallel processing
install.packages("doParallel")
#install.packages("naivebayes")
library('doParallel') # for parallel processing
library('naivebayes') # naive bayes classifier
install.packages("naivebayes")
install.packages("MLmetrics")
#install.packages("naivebayes")
library('doParallel') # for parallel processing
library('naivebayes') # naive bayes classifier
library('MLmetrics') # model performance
# 1. Set grid search for NB classifier - how will you tune your parameters?
tuneGrid = expand.grid(
laplace = c(0,0.5,1.0),
usekernel = c(TRUE, FALSE),
adjust=c(0.75, 1, 1.25, 1.5)) # your code here
# 2. Set up 5-fold cross-validation, repeated 3 times
train_control <- trainControl(method = "repeatedcv",
number = 5,
repeats = 3,
classProbs= TRUE,
summaryFunction = multiClassSummary,
selectionFunction = "best",
verboseIter = TRUE) # your code here
# 3. Set parallel processing cluster
cl <- makePSOCKcluster(5) # create number of copies of R to run in parallel and communicate over sockets
registerDoParallel(cl) # register parallel backed with foreach package
# 4. Train model
nb_train <- train(sentiment ~ ., # fill in formula here
data = Train,
method = "naive_bayes", # fill in method
metric = "F1", # fill in metric to optimise
trControl = train_control,
tuneGrid = tuneGrid,
allowParallel= TRUE
)
pred <- predict(nb_train, newdata = Test)# Your code here
install.packages("kernlab")
