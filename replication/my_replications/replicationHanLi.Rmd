---
title: "Replicating Baerg, N. and Lowe,W.:Textual Taylor Rule"
author: "Han Li"
date: "28/03/2024"
output:
  pdf_document:
    latex_engine: lualatex
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "")
```
## R Setup

```{r, message=FALSE}
library(lme4) 
library(splines) 
library(ggplot2)
library(dplyr) 
library(readr)
library(stargazer)
library(readxl)
library(lubridate)
library(broom)
library(tidyr)
library(cowplot)
library(xtable)

```

During my literature review for QTA, I came across this paper"A Textual Taylor Rule: Estimating Central Bank Preferences Combining Topic and Scaling Methods"by Nicole Baerg and Will Lowe that contains really interesting data so I want to replicate it.

## Introduction and Background:

The Federal Open Market Committee (FOMC) is crucial in U.S. monetary policy, impacting the economy and global finance. It aims to maximize employment, stabilize prices, and moderate long-term interest rates. Balancing these goals, especially when adjusting interest rates, requires careful policy crafting, as it affects inflation, growth, and employment. This complex decision-making backdrop highlights the importance of Taylor's Rule, a guideline that adjusts interest rates based on inflation and output variations. Taylor's Rule Formulation: 
i=r+pi+alpha(pi0−pi_target)+beta(y0−y_current)


Where:
i is the nominal interest rate recommended by the rule.
r is the real neutral interest rate, assumed to be constant.
pi is the current rate of inflation.
pi_target is the target rate of inflation set by the central bank.
y0 is the logarithm of the current real GDP.
y_target is the logarithm of the potential or trend GDP.

α and β are parameters that represent the response of the interest rate to deviations from the target inflation rate and output gap, respectively. Typically alpha = beta =0.5 is considered a 'balanced' policy.

This rule provides a simple yet powerful framework to guide central banks in adjusting interest rates to stabilize the economy by addressing inflation and output variations.
The objective of the paper is to develop a method combining topic-based text analysis and scaling methods to estimate the preferences of US Federal Open Market Committee (FOMC) members without relying on voting data based on taylor's rule expressed in their texts.
## Paper Abstract:
Scholars often use voting data to estimate central bankers’ policy preferences but consensus voting is commonplace. To get around this, we combine topic-based text analysis and scaling methods to generate theoretically motivated comparative measures of central bank preferences on the US Federal OpenMarket Committee (FOMC) leading up to the financial crisis in a way that does not depend on voting behavior. We apply these measures to a number of applications in the literature. For example, we find that
FOMC members that are Federal Reserve Bank Presidents from districts experiencing higher unemployment are also more likely to emphasize unemployment in their speech. We also confirm thatcommittee members on schedule to vote are more likely to express consensus opinion than their off schedule voting counterparts.

## Methodology: 
The study uses a combination of Latent Dirichlet Allocation (LDA) for topic modeling and positional analysis to interpret the central bankers' discussions within the framework of the Taylor rule. Whereas I have included replication results for the figures, this is not in the scope of this course. I will concentrate on the statistic model components.

## Statistical Models Used:
### Model 1: 
They model the counts of words and phrases in the inflation and output topics (c1 and c2), respectively, as:
[c1; c2] ~ Binomial(p;N)
p = P(c1| N)
N = c1 + c2
Where p is the probability of 'inflation-related'speech.
position estimates:
log (p_t/1 -p_t) = intercept + speaking +meeting random effects(error term) :

Model Assumpions:
assume that speakers’ positions are exchangeable and model them as draws from a population of
committee members.
Topics not related to inflation or output can be ignored because they give no information
about β1 = β2.


## Replication: 
### Model 1

There are 4 figures relevant in this part of the model: *figure 1* is a descriptive figure as motivation, *figure 2* is the main finding, *figure 3 and 4* are validation (comparing finding to external sources).

### Figure 1:descriptive plot: member consensus

```{r}
read_excel("FOMC_Dissents_Data.xlsx", skip = 3, col_names = TRUE) %>% 
  filter(Year >= 2005, Year <= 2007) %>% 
  group_by(`FOMC Meeting`) %>% 
  mutate(diss = `Number Presidents Dissenting` + `Number Governors Dissenting`) %>% 
  summarise(total = `FOMC Votes`, Assents = total - diss, Dissents = diss) %>% 
  gather(Direction, Votes, -c(`FOMC Meeting`, total)) %>% 
  ggplot(aes(x = `FOMC Meeting`, y = Votes, colour = Direction, fill = Direction)) + 
  geom_bar(stat = "identity") + 
  scale_colour_manual(values = c("grey", "black")) +
  scale_fill_manual(values = c("grey", "black")) +
  scale_y_continuous("Votes Cast", breaks = 0:12, labels = as.character(0:12)) + 
  theme_minimal()

 ggsave("dissentsplot.pdf", width = 7, height = 5)
```
```{r, results = "hide"}
ipplot <- function(ideal, speaker, lower, upper){
  row <- 1:length(ideal)
  p <- ggplot(df, aes(ideal, speaker)) +
    geom_point(size = 2) +
    geom_segment(aes(x=lower, xend=upper, y=row, yend=row)) +
    theme_minimal() +
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor = element_blank())
  p
}
```

### Figure 2 is the main finding.
This lists the names of FOMC members per year that we care about. 

```{r, echo = TRUE}
nicole_peeps2007 <- c("GREENSPAN", "GEITHNER", "BIES", "EVANS", 
                      "FISHER", "FERGUSON", "GUYNN", "KOHN", 
                      "LOCKHART", "LACKER", "PIANALTO", "YELLEN", 
                      "MINEHAN", "MOSKOW", "POOLE", "HOENIG", 
                      "FISHER", "STERN", "SANTOMERO")
nicole_peeps2006 <- c("BERNANKE", "GEITHNER", "BIES", "HOENIG", 
                      "KOHN", "KROSZNER", "MINEHAN", "MISHKIN", 
                      "MOSKOW", "POOLE", "WARSH", "FISHER", "OLSON",
                      "PIANALTO", "PLOSSER", "STERN", "STONE", 
                      "LACKER", "YELLEN")
nicole_peeps2005 <- c("GREENSPAN", "GEITHNER", "BERNANKE", "BIES", 
                      "FERGUSON", "FISHER", "GRAMLICH", "KOHN", 
                      "MOSKOW", "OLSON", "SANTOMERO", "STERN", 
                      "GUYNN", "LACKER", "PIANALTO", "YELLEN", 
                      "HOENIG", "MINEHAN", "POOLE", "HOLCOMB", 
                      "CUMMING")
nicole_peeps <- unique(c(nicole_peeps2007, nicole_peeps2006, 
                         nicole_peeps2005))
```

This is the meeting data and the topic output 

```{r}
# meeting data and speaker metadata
ddm <- read.csv("name_role_date.csv")
# topic model output
c_file <- 'topical-ngrams-document-topics.csv'
dd1 <- read.csv(c_file, sep = '\t')[,-1]
names(dd1) <- 0:24 # these are the topics, from 1 to 25
```

Now to pick out the topics we are interested in (we do this by hand i.e. determine which topics):

 - 7+17+21 (employment / output)
 - 8 (core inflation) (9, 14, 15: energy + house prices)

Let's have them as constants:
```{r}
# full sample left and right categories
R_cats_full <- '8'
L_cats_full <- c('7', '17', '21')

## and a function to create an appropriate DV with such things
mk_dv <- function(mat, right, left, na.omit = FALSE){
  res <- data.frame(R = rowSums(mat[, right, drop = FALSE]), 
                    L = rowSums(mat[, left, drop = FALSE]))
  if (na.omit)
    return(na.omit(res))
  res
}

# switch dd to the numerical column labels
# names(dd) <- c('7', '17', '21', '8')
dd <- dd1[,c('7','17','21','8','9','14','15')]
```


```{r}
good <- rowSums(dd) > 0
meta <- filter(ddm, good, name %in% nicole_peeps) 
counts <- filter(dd, good, ddm$name %in% nicole_peeps) 
```


In these regressions the dependent variable is two counts for each speaker-meeting

> [successes (right, inflation talk), failures (left, unemployment-output talk)]

```{r}
# collapse over three topic counts (see Lowe et al. 2011) and regress
depvar <- mk_dv(counts, R_cats_full, L_cats_full)
meta <- meta %>% 
  mutate(date = dmy(date), month = month(date, label = TRUE),
         year = year(date))
```

Linear mixed effects model

Models for both the Bank Presidents and also the Board Members 
```{r, results="hide"}
mod.mix <- glmer(as.matrix(depvar) ~ (1 | name) + (1 | month), 
                 data = meta, family = binomial)
mod.mix
```


```{r, results="hide"}
rf <- ranef(mod.mix, condVar = TRUE)$name
rf_postvar <- as.vector(attr(rf, "postVar"))
df <- data.frame(ideal = rf[[1]]) %>%
  mutate(se = sqrt(rf_postvar),
         upper = ideal + 2 * se,
         lower = ideal - 2 * se,
         speaker = factor(rownames(rf), 
                          levels = rownames(rf)[order(ideal)])) %>%
  arrange(ideal)
```

Estimated Fixed Ideal Points from Full Transcripts with meeting random effects.

This is Figure 2 from the paper

```{r, echo = TRUE}
ipplot(df$ideal, df$speaker, df$lower, df$upper) + 
     labs(x = "Ideal point", y = "Speaker")
```
Replicated Fig. 2. Estimated fixed ideal points from full transcripts with meeting random effects. Dovish members are those occupying
the left hand side of the scale and Hawkish members are those on the right hand side.

### Figure 3 & 4, validation against different data sources.

```{r}
merged_votes_text <- read.csv("merged_votes_text.csv", 
                              row.names = 1, stringsAsFactors = FALSE)
```
Expert Order of the FT Ranking is: 

```{r}
ft_coding <- data.frame(Name = c("Kohn", "Yellen", "Bernanke", "Pianalto",
                                "Fisher", "Lacker", "Plosser", "Hoenig"),
                                 coding = c("Super Dove", "Dove", "Center", "Center", 
                                "Hawk", "Super Hawk", "Super Hawk", "Super Hawk"), 
             stringsAsFactors = FALSE)
```

These are the votes by Eijffinger, Sylvester CW, Ronald Mahieu, and Louis Raes. "Hawks and Doves at the FOMC." (2015), kindly provided by the authors. 

```{r}
votes.implied <- merged_votes_text[,6:9]
names(votes.implied) <- c("Names", "Low.Votes", "Estimate", "High.Votes")
```

This is the regional information:

The regional estimates are from here Bennani, Hamza, Etienne Farvaque, and Piotr Leszek Stanek. "FOMC members’ incentives to disagree: regional motives and background influences." (2015).

```{r}
votes.regional <- read.csv("policy_rate_validation.csv", 
                           stringsAsFactors = FALSE, row.names = 1) %>%
  mutate(NAME = toupper(name))
```

```{r}
est <- read.csv("idealcompare.csv", 
                stringsAsFactors = FALSE, row.names = 1)

tdf <- merged_votes_text %>% 
  left_join(votes.regional, by = "NAME") %>%
  left_join(est, by = "NAME") %>%
  mutate(Pref.low = Pref.Policy - 1.96 * Pref.STD,
         Pref.high = Pref.Policy + 1.96 * Pref.STD,
         med.votes.sd = (med.votes - high.votes) / 1.96) %>%
  arrange(ideal) %>%
  mutate(ordered = 1:n())
  

small_df <- tdf %>% 
  right_join(ft_coding, by = "Name") %>% 
  select(NAME, ideal, upper, lower, coding) %>% 
  mutate(coding = factor(coding, 
                         levels = c("Super Dove", "Dove", "Center", "Hawk", "Super Hawk"))) %>%
  arrange(coding, ideal) %>%
  mutate(row = 1:n())
```

FT rankings and ideal points plot

### This is Figure 3

```{r}
labs1 <- small_df$NAME
labs2 <- small_df$coding

ggplot(small_df, aes(x = ideal, y = row)) + 
  geom_point(size = 2) +
  geom_segment(aes(x = lower, xend = upper, y = row, yend = row)) +
  scale_y_continuous(breaks = 1:length(labs1),
                     labels = labs1,
                     sec.axis = sec_axis(~.,
                                         breaks = 1:length(labs2),
                                         labels = labs2)) +
  theme_minimal() +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor = element_blank(),
        legend.title = element_blank(),
        legend.background = element_blank()) + 
   labs(x = "Ideal point", y = "Speaker") 

# ggsave("rank.pdf", width = 7, height = 5)
```
```{r}
nms <- tdf$NAME
g_text <- ggplot(tdf, aes(x=ideal, y=ordered)) + 
  geom_point() + 
  geom_errorbarh(aes(xmin = lower, xmax = upper, height = 0)) + 
  theme_bw() + 
  scale_y_continuous(breaks = 1:length(nms),
                     labels = nms) + 
  theme(axis.title = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.minor = element_blank(),
        plot.title = element_blank()) +
        #axis.text.y = element_text(face = c(rep("plain",15),rep("bold",8)))) + 
  ggtitle("Text Measure") 

g_votes <- ggplot(tdf, aes(x=med.votes, y=ordered)) + 
  geom_point() + geom_errorbarh(aes(xmin = low.votes, xmax = high.votes, height = 0)) + 
  theme_bw() + 
  theme(axis.text.y = element_blank(), axis.title = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.minor = element_blank(),
        plot.title = element_blank()) +  
  ggtitle("Vote Measure") + 
  xlim(-4,4)

g_region <- ggplot(tdf, aes(x=Pref.Policy, y=ordered)) + 
  geom_point() + geom_errorbarh(aes(xmin = Pref.Policy - 1.96*Pref.STD, 
                                    xmax = Pref.Policy + 1.96*Pref.STD, height = 0 )) + 
  theme_bw() + 
  theme(axis.text.y = element_blank(), axis.title = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.minor = element_blank(),
        plot.title = element_blank()) + 
  
  ggtitle("Regional Vote Measure") + 
  xlim(-10,10)
```
### This is Figure 4

```{r, echo = FALSE}
plot_grid(g_text, g_votes, g_region, 
          align = "h", ncol = 3, 
          rel_widths = c(0.38, 0.31, 0.31))
```

# Model 2: 
This part contains *1 table* to replicate. It tests the hypothes that the policy preferences of FOMC members are influenced by the economic conditions of their respective home districts and their personal career motivations, particularly their connections to the financial sector. The hypothesis suggests that these factors may lead members to exhibit a bias toward hawkish or dovish policy stances in their speeches. It employs a mixed-effects logistic regression model to analyze the relationship between the economic conditions and financial sector indicators and the policy stance point obtained. 
**Explanatory Variables:**
1. Regional economic conditions, represented by:
   - The regional unemployment rate.
   - The difference between district-level and national-level unemployment.
   - The logged dollar amount of non-performing loans in a member's district, serving as a proxy for the financial sector's size.
2. National economic indicators, including:
   - Current-period national inflation rate.
   - Current-period national unemployment rate.
3. Future economic projections:
   - One-year-ahead projected national inflation.
   - One-year-ahead projected national unemployment.
**Response Variable:**
The proportion of speech content dedicated to discussing inflation compared to unemployment and output, reflecting each FOMC member's policy inclination or preference.

**Assumptions:**
1. Linearity in the Logit: The relationship between the logit of the response variable and the explanatory variables is assumed to be linear.
2. Independence: Observations are assumed to be independent within groups, though the model accounts for repeated measures from the same individuals over time.
3.No Perfect Multicollinearity: The explanatory variables should not be perfectly collinear, ensuring the model's estimates are reliable.
4. Random Effects Distribution: Random effects are assumed to follow a normal distribution, contributing to the variability in policy preferences among FOMC members.
5. Proportional Odds: The odds ratios for the categories of the fomc policy stance are assumed to be proportional and consistent across different levels of the explanatory variables.

## Replicating model 2: adding in the economic covariates 

```{r, results="hide"}
regional <- read_excel("GreenbookForecastsRegional.xlsx") %>%
  mutate(reg_un = `Regional Unemployment in Month of Meeting`,
         nat_un = `Projected Unemployment Current`,
         nat_un_q4 = `Projected Unemployment 4-Q AH`,
         nat_inf = `Projected Inflation Current Q CPI`,
         nat_inf2 =  `Projected Inflation Current Q PCE`,
         nat_inf_q42 = `Projected Inflation 4-Q AH PCE`,
         nat_inf_q4 = `Projected Inflation 4-Q AH CPI`,
         diff = nat_un - reg_un,
         logit = log(nat_un / reg_un),
         npl = as.numeric(amount), #non-performing loans 
         date = as.Date(Meeting)) %>%
  filter(District != 0)
``` 

1. Model with national inflation + regional (district) unemployment + differennce between national and regional

```{r, results="hide"}
dat <- bind_cols(meta, depvar) %>%
  left_join(regional, ., by = c('name', 'date'))
mod.econ <- glmer(cbind(R, L) ~ (1 | name) + nat_inf + nat_un + diff , data = dat, family = binomial)

```

```{r, results="hide"}
mod.econ2 <- glmer(cbind(R, L) ~ (1 | name) + log(npl) + nat_un + diff, data = dat, family = binomial)

```

```{r, results="hide"}
mod.econ3 <- glmer(cbind(R, L) ~ (1 | name) + nat_inf_q4 + nat_un_q4 + diff,  data = dat, family = binomial)

```

The source of the CPI data is the Bureau of Labor Statistics http://www.usinflationcalculator.com/inflation/historical-inflation-rates/

```{r}
bls_cpi_inflation <- c(2.1,2.4,	2.8,	2.6,	2.7,	2.7,	2.4,	2.0,	2.8,	3.5,	4.3,	4.1, #2005
          4.0,	3.6,	3.4,	3.5,	4.2,	4.3,	4.1,	3.8,	2.1,	1.3,	2.0,	2.5,	#2006
          3.0,	3.0,	3.1,	3.5,	2.8,	2.5,	3.2,	3.6,	4.7,	4.3,	3.5,	3.4)	#2007
```

Okay, now putting these things into a table, we get the following:

### This is Table 1 

```{r}
stargazer(mod.econ, mod.econ2, mod.econ3, type="text",
          title="Regression Results for FOMC Bank Presidents", single.row=TRUE,
          ci=TRUE, ci.level=0.9, omit.stat=c("f", "ser"))
```
```{r}
dat2 <- read_excel("ideal_date.xlsx") %>%
  mutate(dissent = ifelse(Consent.Policy.Vote == 0, 1, 0),
         vote = if_else(Vote == "y", 1, 0),
         statement = if_else(Consent.Policy.Statement == 0, 1, 0))

mod.1 <- glmer(dissent ~  est  + vote + (1 | date), data = dat2, family = binomial)
mod.2 <- glmer(statement ~ est +  vote +  (1 | date), data = dat2, family = binomial)

summary(mod.1)
summary(mod.2)
```

### This is Table 2

```{r}
stargazer(mod.1, mod.2, type="text", title="Results", 
          align=TRUE, dep.var.labels=c("Dissent Policy Rate","Dissent Statement"))
```
# Adding my twist:
Challenging the assumption that the other topics don't carry information regard policy preference- FOMC has three mandate: keeping inflation low (hawkish/ inflation topic), maximising employment(dovish/ output and employment topic), and keeping financial stability (neutral, the rest of the topic)- > taking the share of inflation/ employment topic in total speeches intead of dropping the non-selected topics
```{r}
# adding another category, the ones to regarded as neutral
R_cats_full <- c('8','9','14','15')
L_cats_full <- c('7', '17', '21')
N_cats_full <- c('0','1','2','3','4','5','6','10','11', '12', '13', '16', '18', '19', '20', '22', '23', '24')
mk_dv_modified <- function(mat, right, left, middle, na.omit = FALSE) {
  res <- data.frame(
    R = rowSums(mat[, right, drop = FALSE]),
    L = rowSums(mat[, left, drop = FALSE]),
    M = rowSums(mat[, middle, drop = FALSE])
  )
  if (na.omit) {
    return(na.omit(res))
  }
  res
}
counts_full <- filter(dd1, ddm$name %in% nicole_peeps) 
depvar_multi <- mk_dv_modified(counts_full, R_cats_full, L_cats_full,N_cats_full)
#convert them into weight
# Add a new column to store the sum of R, L, and M for each row
depvar_multi$total <- depvar_multi$R + depvar_multi$L + depvar_multi$M

# Calculate the weight of each column (R, L, M) as a proportion of the total
depvar_multi$R_weight <- depvar_multi$R / depvar_multi$total
depvar_multi$L_weight <- depvar_multi$L / depvar_multi$total
depvar_multi$M_weight <- depvar_multi$M / depvar_multi$total

# Now depvar_multi includes the weights of R, L, and M in addition to the original values
#use the weight after taking into account of the neutral speaches of the l/r
depvar_weihgt=depvar_multi[,c('R_weight','L_weight')]
meta_multi <- filter(ddm, name %in% nicole_peeps) 
meta_multi <- meta_multi %>% 
    mutate(month = month(date, label = TRUE),
           year = year(date))
mod.new <- glmer(as.matrix(depvar_weihgt*100) ~as.factor(year)+ (1 | name) + (1 | month), 
                 data = meta_multi, family = binomial)
mod.new
```
## plotting figure 2 (main finding)..

```{r}
rf_new <- ranef(mod.new, condVar = TRUE)$name
rf_newvar <- as.vector(attr(rf_new, "postVar"))
dfnew <- data.frame(newideal = rf_new[[1]]) %>%
  mutate(se = sqrt(rf_newvar),
         upper_new = newideal + 2 * se,
         lower_new = newideal - 2 * se,
         speaker = factor(rownames(rf_new), 
                          levels = rownames(rf_new)[order(newideal)])) %>%
  arrange(newideal)
ipplot_new <- function(dfp, speaker, ideal, ideal_new, lower, upper, lower_new, upper_new) {
  # Combine the original and new ideal values into a long format
  df_long <- tidyr::pivot_longer(dfp, 
                                 cols = c(ideal, ideal_new),
                                 names_to = "type",
                                 values_to = "ideal_value")
  
  # Combine the original and new lower and upper values similarly
  df_long$lower <- ifelse(df_long$type == "ideal", df_long[[lower]], df_long[[lower_new]])
  df_long$upper <- ifelse(df_long$type == "ideal", df_long[[upper]], df_long[[upper_new]])
  
  # Create the plot
  p <- ggplot(df_long, aes(x = ideal_value, y = speaker, color = type)) +
    geom_point(size = 2) +
    geom_segment(aes(x = lower, xend = upper, y = speaker, yend = speaker)) +
    theme_minimal() +
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor = element_blank()) +
    scale_color_manual(values = c("blue", "red")) # Set your desired colors here

  return(p)
}
dfmerge=merge(df,dfnew,by='speaker')

# Call the function like this:
ipplot_new(dfmerge, "speaker", "ideal", "newideal", "lower", "upper", "lower_new", "upper_new")+
labs(x = "Ideal points", y = "Speaker")
#ipplot(dfnew$newideal, dfnew$speaker, dfnew$lower, dfnew$upper) + 
 #    labs(x = "Ideal point new", y = "Speaker")
```
and now do the regression:
```{r, results="hide"}
dat <- bind_cols(meta_multi, depvar_weihgt*100) %>%
  mutate(date = dmy(date)) %>%
  left_join(regional, ., by = c('name', 'date'))
mod.econ <- glmer(cbind(R_weight, L_weight) ~ (1 | name) + nat_inf + nat_un + diff , data = dat, family = binomial)

```
```{r, results="hide"}
mod.econ2 <- glmer(cbind(R_weight, L_weight) ~ (1 | name) + log(npl) + nat_un + diff, data = dat, family = binomial)

```
```{r, results="hide"}
mod.econ3 <- glmer(cbind(R_weight, L_weight) ~ (1 | name) + nat_inf_q4 + nat_un_q4 + diff,  data = dat, family = binomial)

```
```{r}
stargazer(mod.econ, mod.econ2, mod.econ3, type="text",
          title="Regression Results for FOMC Bank Presidents", single.row=TRUE,
          ci=TRUE, ci.level=0.9, omit.stat=c("f", "ser"))
```